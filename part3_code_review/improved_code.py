# -*- coding: utf-8 -*-
"""improved_code

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ubvQ2KerBQGGntz7HWPUR-GQe4wctfE1
"""

import torch
import torch.nn as nn


class TransformerModel(nn.Module):

    def __init__(
        self,
        vocab_size: int,
        d_model: int,
        nhead: int,
        num_layers: int,
        max_len: int = 512,
        dropout: float = 0.1,
        pad_id: int = 0,
    ):
        super().__init__()
        self.pad_id = pad_id

        # Token and position embeddings
        self.embedding = nn.Embedding(vocab_size, d_model, padding_idx=pad_id)
        self.pos_embedding = nn.Embedding(max_len, d_model)

        # Transformer encoder
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=nhead,
            dropout=dropout,
            batch_first=True,  # expects (batch, seq, d_model)
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)

        # Output projection
        self.fc = nn.Linear(d_model, vocab_size)

    def forward(self, x: torch.Tensor, key_padding_mask: torch.Tensor | None = None) -> torch.Tensor:
        if x.dtype != torch.long:
            raise TypeError("Input must be token indices (torch.long).")

        bsz, seq_len = x.shape
        if seq_len > self.pos_embedding.num_embeddings:
            raise ValueError("Sequence length exceeds maximum supported length.")

        positions = torch.arange(seq_len, device=x.device).unsqueeze(0)

        # Embed tokens + positions
        x = self.embedding(x) + self.pos_embedding(positions)

        # Transformer encoder
        x = self.transformer(x, src_key_padding_mask=key_padding_mask)

        # Vocabulary logits
        return self.fc(x)


def train_model(
    model: nn.Module,
    data,
    epochs: int = 10,
    lr: float = 3e-4,
    pad_id: int = 0,
    log_every: int = 50,
    device: torch.device | None = None,
):
    device = device or torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)
    model.train()

    optimizer = torch.optim.Adam(model.parameters(), lr=lr)
    loss_fn = nn.CrossEntropyLoss(ignore_index=pad_id)

    step = 0
    for epoch in range(1, epochs + 1):
        for batch in data:
            if len(batch) == 2:
                inputs, targets = batch
                key_padding_mask = None
            elif len(batch) == 3:
                inputs, targets, key_padding_mask = batch
            else:
                raise ValueError("Batch must be (inputs, targets) or (inputs, targets, key_padding_mask).")

            inputs = inputs.to(device)
            targets = targets.to(device)
            if key_padding_mask is not None:
                key_padding_mask = key_padding_mask.to(device)

            optimizer.zero_grad(set_to_none=True)

            outputs = model(inputs, key_padding_mask=key_padding_mask)
            loss = loss_fn(
                outputs.reshape(-1, outputs.size(-1)),
                targets.reshape(-1),
            )

            loss.backward()
            optimizer.step()

            step += 1
            if step % log_every == 0:
                print(f"epoch={epoch} step={step} loss={loss.item():.4f}")


# Training
vocab_size = 10000
model = TransformerModel(vocab_size, d_model=512, nhead=8, num_layers=6)
train_model(model, train_data)